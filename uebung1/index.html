<!DOCTYPE HTML>
<htmL>
  <head>
    <title>MTA SS2014 &Uuml;bung 1</title>
    <link rel="stylesheet" type="text/css" href="style.css">
  </head>
  <body>


<h2>MTA Aufgabe 1</h2>
<h4>Victoria K&ouml;hring - 798552<br/>Tom Oberhauser - 798158</h4>
<br/><br/>

<h3>1. Aufgabe</h3>
<p class="aufgabe">
  Erzeuge zwei kurze Audio-Files (max. 20 s), davon eines mit Music von einer CD deiner Wahl (wobei sich Music mit
  einer relativ hohen Dynamik, d.h. Wechsel zwischen relativ leisen und lauten Abschnitten empfiehlt). W&auml;hle
  eine geeignete Abtastfrequenz (begr&uuml;nden !) und achte auf gute Aussteuerung. Das zweite Audio-File soll eine
  Sprachaufnahme (mit dem Headset aufgesprochen) enthalten (auf &Uuml;bersteuerung achten !). W&auml;hle hier eine
  Abstastfrequenz von 22 kHz, 16 bit Aufl&ouml;sung, mono. Die Einstellungen wie Abtastrate, Bitzahl und Kanalzahl
  k&ouml;nnen in Wavestudio Samplitude vorgenommen werden. Die Eingangsquelle (wahlweise Audio-CD oder Mikrofon) kann
  im Windows-Mixer 'Aufnahme' eingestellt werden. Lies beide Wave-Files mit wave_io ein und erkl&auml;re die Angaben
  im Header ! Wie hoch ist die Bitrate f&uuml;r die beiden Dateien?
</p>
<p class="antwort">
Die Headerdaten f&uuml;r unsere Youtube bzw. Sprachaufnahme sind hier erl&auml;utert. Die Bitrate errechnet sich wie folgt: <br/><br/>
"rate" * "bits" * "channels" = Bitrate (Bits pro Sekunde)

<table>
  <tr>
    <th>infile</th>
    <th>filelength</th>
    <th>samples</th>
    <th>rate</th>
    <th>bits</th>
    <th>bytes per sample</th>
    <th>channels</th>
    <th><i>(Bitrate)</i></th>
  </tr>
  <tr>
    <td><a href="files/youtube.wav">youtube.wav</a></td>
    <td>2954998</td>
    <td>1476422</td>
    <td>44100</td>
    <td>16</td>
    <td>4</td>
    <td>2</td>
    <td>1.411.200 Bits pro Sekunde</td>
  </tr>
  <tr>
    <td><a href="files/aufnahme.wav">aufnahme.wav</a></td>
    <td>3807776</td>
    <td>1903150</td>
    <td>44100</td>
    <td>16</td>
    <td>2</td>
    <td>1</td>
    <td>705.600 Bits pro Sekunde</td>
  </tr>
</table>

<br/><b>Erkl&auml;ung der Angaben:</b><br/><br/>
<b>filelength</b> ist die Dateil&auml;nge in Byte<br/>
<b>samples</b> ist die Anzahl Samples. Diese setzt sich zusammen aus <i>filelength</i> dividiert durch <i>bytes per sample</i> <br/>
<b>rate</b> ist die Abtastfrequenz in Hertz<br/>
<b>bits</b> Anzahl der Bits pro Kanal und Sample. In diesem Fall 16 Bit, also insgesamt 65536 Abtastwerte.<br/>
<b>bytes per sample</b> Anzahl der bytes pro Sample f&uuml;r alle Kan&auml;le in unserem Fall bei mono = 16 Bit * 1 = 2 Byte, f&uuml;r Stereo 16 Bit * 2 = 4 Byte<br/>
<b>channels</b> Anzahl der Tonkan&auml;le. Bei Mono = 1, bei Stereo = 2

</p>

<h3>2. Aufgabe - Aliasing</h3>
<h4>2.1</h4>
<p class="aufgabe">
  Modifiziere wave_io dahingehend, dass die Samples in der WAV-Datei in eine (lesbare) ASCII-Datei geschrieben
  werden. Lies die Dateien sine_lo.wav und sine_hi.wav
  (Sampling-Frequenz: 16 kHz) ein und bestimme aus den resultierenden Zahlenfolgen in der ASCII-Datei
  die Frequenz der Sinus-Schwingungen (Begr&uuml;nden und jeweils eine Periode f&uuml;r das Protokoll abspeichern).
  &Uuml;berpr&uuml;fe Deine Sch&auml;tzung mit dem Spektralanalyse-Tool GRAM (Plots ins Protokoll !).
  Vorgehensweise: Men&uuml;punkt Analyze File, Einstellungen: Freq Scale: Linear, FFT Size: 512, Time scale: 1 msec)
<p>
<p class="antwort">
wave_io wurde um folgende Zeilen erweitert:<br/>
<pre>
prt=fopen("prt.txt","wt");
for(i=0; i&lt;n_wave; i++) {
  fprintf(prt, "%i\n", wave[i]);
}
fclose(prt);
</pre>
<br/>
<b>sine_hi:</b>
<pre>
0
10606
-15000
10606
0
-10606
15000
-10606
0
...
</pre>
Zwischen den beiden 0 Samples finden 3 Nulldurchg&auml;nge statt. Das bedeutet in 4 Samplewerten befinden sich 1,5 Schwingungen. 4 Samplewerte haben eine Dauer von 16000Hz / 4 , also 1/4000 Sekunde. In 1/4000 Sekunde finden 1,5 Schwingungen statt, das bedeutet eine Schwingung dauert 1/4000 * 1,5 = 1/6000 Sekunde. Das entspricht 6000Hz.<br/>
<img src="img/gram_sine_hi.jpg">
<br/><br/>
<b>sine_lo:</b>
<pre>
0
13858
10606
-5740
-15000
-5740
10606
13858
0
...
</pre>
Zwischen den beiden 0 Samples finden 3 Nulldurchg&auml;nge statt. Das bedeutet in 8 Samplewerten befinden sich 1,5 Schwingungen. 8 Samplewerte haben eine Dauer von 16000Hz / 8 , also 1/2000 Sekunde. In 1/2000 Sekunde finden 1,5 Schwingungen statt, das bedeutet eine Schwingung dauert 1/2000 * 1,5 = 1/3000 Sekunde. Das entspricht 3000Hz.<br/>
<img src="img/gram_sine_lo.jpg">

</p>

<h4>2.2</h4>
<p class="aufgabe">
  Bei der zeitlichen Diskretisierung eines Analogsignals mu&szlig; das sogenannte Abtasttheorem eingehalten werden.
  Wie lautet es und wie l&auml;&szlig;t sich der Grenzfall, f&uuml;r den es gerade noch gilt,
  illustrieren (Zeichnung !)?
<p class="antwort">
  Das Abtasttheorem besagt dass die Abtastfrequenz gr&ouml;sser als die doppelte, im abzutastenden Signal vorhandene
  h&ouml;chste Frequenz, sein muss.
  <br/><br/><img src="img/abtasttheorem.png" height="20"><br/><br/>
  Falls die Abtastfrequenz exakt der doppelten H&ouml;chstfrequenz entspricht
  KANN eine Abtastung m&ouml;glich sein wenn man genau die Maximalausschl&auml;ge der Schwingung trifft (In Illustration gr&uuml;n dargestellt).
  Im ung&uuml;nstigsten Fall ist die Schwingung jedoch phasenverschoben, sodass man nicht die
  Maximalausschl&auml;ge, sondern genau die Nulldurchg&auml;nge abtastet. (In Illustration rot dargestellt).
  <br/><br/><img src="img/illustration_abtasttheorem.png"><br/><br/>
</p>

<h4>2.3</h4>
<p class="aufgabe">
  Bei herk&ouml;mmlichen Soundkarten tritt systembedingt kein Aliasing auf, weil das Audiosignal stets geeignet
  vorbehandelt wird (wie ?). Mit einem kleinen Trick l&auml;&szlig;t sich Aliasing jedoch nachweisen.
  Diese auch als Down-Sampling bekannte Methode besteht darin, dass man bei einer WAV-Datei z.B. jeden zweiten
  Abtastwert wegwirft. Man erh&auml;lt so eine Wellenform, die genau die H&auml;lfte der urspr&uuml;nglichen
  Abtastfrequenz aufweist. Wenn man das Signal nicht vorher bandbegrenzt hat, k&ouml;nnen Aliasing-Verzerrungen
  h&ouml;rbar werden.
</p>
<p class="antwort">
  Vor der Digitalisierung wird in der Soundkarte ein Tiefpassfilter angewendet, der sicherstellt dass Frequenzen gr&ouml;&szlig;ergleich der Abtastfrequenz nicht mehr im Signal existieren. Dadurch wird das Abtasttheorem eingehalten.
</p>

<h4>2.4</h4>
<p class="aufgabe">
  Modifiziere wave_io dahingehend, dass vom eingelesenen Signal jeder zweite Abtastwert verworfen wird und das
  resultierende Signal abgespeichert wird. Der Header mu&szlig; nat&uuml;rlich entsprechend ver&auml;ndert werden!
  Wende das resultierende Programm zun&auml;chst auf 'sine_lo.wav' und 'sine_hi.wav' an. Welche Frequenzen erscheinen
  nach dem Down-Sampling (Spektrogramm und WAVs ins Protokoll !)? Was w&uuml;rde passieren, wenn man geeignet
  bandbegrenzen w&uuml;rde?
</p>
<p class="antwort">
wave_io wurde um folgenden Code erweitert:
<pre>
for(i=0; i < n_wave/2; i++) {
	wave[i]=wave[i*2];
}
n_wave/=2;
freq_in/=2;
</pre>
<a href="files/sine_lo_down.wav">sine_lo_down.wav</a><br/>
<a href="files/sine_hi_down.wav">sine_hi_down.wav</a><br/>
<img src="img/gram_sine_lo_down.jpg">
<img src="img/gram_sine_hi_down.jpg"><br/><br/>
Die Datei sine_lo_down.wav entspricht der Originaldatei sine_lo.wav da die Ausgangsfrequenz von 3Khz das Abtasttheorem mit der neuen Abtastfrequenz von 16Khz/2 = 8Khz weiterhin erf&uuml;llt. Bei sine_hi_down.wav sehen wir Aliasing, da das Abtasttheorem f&uuml;r eine Tonfrequenz von 6Khz und einer Abtastfrequenz von 8Khz nicht mehr eingehalten wird. Daraus resultiert eine Spiegelung der Frequenz an der halben Abtastfrequenz, also 6Khz - 4Khz = 2Khz. Wenn man entsprechend bandbegrenzen w&uuml;rde, w&uuml;rde man in der Datei sine_hi_down.wav nichts mehr h&ouml;ren, da alle Frequenzen oberhalb von 4Khz (8Khz / 2 ) vor der Digitalisierung entfernt worden w&auml;ren.

</p>

<h3>3. Aufgabe - Bitreduzierung</h3>
<h4>3.1</h4>
<p class="aufgabe">
  Die herk&ouml;mmlichen PC-Soundkarten arbeiten meist entweder mit 16 oder 8 bit-Aufl&ouml;sung.
  Wie gro&szlig; ist die Anzahl der bei diesen beiden Werten darstellbaren Amplitudenwerten ?
</p>
<p class="antwort">
  8 bit entspricht 2^8 Amplitudenwerten = 256 Amplitudenwerte = von -128 bis 127<br/>
  16 bit entspricht 2^16 Amplitudenwerten = 65536 Amplitudenwerte = von -32768 bis 32767
</p>

<h4>3.2</h4>
<p class="aufgabe">
  Wir wollen nun wave_io so modifizieren, dass wir die Bitzahl reduzieren k&ouml;nnen.
  Dazu k&ouml;nnen wir z.B. alle Samples durch eine Potenz von 2 teilen (Integer-Division ohne Rest).
  Damit das resultierende Signal nicht leiser wird als das Original, kompensieren wir die Operation durch
  Multiplikation mit derselben Zweierpotenz. Zu beachten: Der Datentyp hat nach wie vor 16 bit!
  (Denselben Effekt erreicht man auch durch einfaches logisches 'Verunden' mit einem entsprechenden HEX-Wert,
  indem man mit dem LSB beginnend Bits 'ausblendet'.)
</p>
<p class="antwort">
  in wave_io wurde folgender Code eingef&uuml;gt:
  <pre>
  read_wave(&wave, &n_wave, &freq_in, &bits_in, in_name, &header);
  for(i=0; i < n_wave; i++) {
	wave[i] /= pow(2,10);
	wave[i] *= pow(2,10);
  }
  </pre>
</p>

<h4>3.3</h4>
<p class="aufgabe">
  Mit dem entstandenen Programm ver&auml;ndern wir die in Aufgabe 1 erzeugten Wave-Dateien.
  Ab welcher Bitzahl tritt bei Musik/Sprache eine h&ouml;rbare/deutliche Verschlechterung der Qualit&auml;t ein?
  Bei wieviel Bit ist das Sprachsignal noch verst&auml;ndlich ?
  (Waves f&uuml;r all diese F&auml;lle ins Protokoll, Ausschnitte als Plots)
Was charakterisiert das entstehende Quantisierungsger&auml;usch und macht es besonders st&ouml;rend?
</p>
<p class="antwort">
  Bei Musik tritt bei einer <a href="files/youtube_bitreduzierung2hoch10.wav">Bitreduktion von 10 Bit</a> Eine deutliche Verschlechterung der Qualit&auml;t auf.<br/><br/>
  <img src="img/youtube_bitreduzierung2hoch10.png"/><br/><br/>
  Das Sprachsignal ist bei <a href="files/aufnahme-2hoch11.wav">11 Bit Reduktion</a> gerade noch verst&auml;ndlich.<br/>
  <img src="img/aufnahme_2hoch11.jpg"/><br/><br/>
  <a href="files/aufnahme-2hoch12.wav">Bei 12 Bit jedoch nicht mehr.</a><br/>
  <img src="img/aufnahme_2hoch12.jpg"/><br/>
Das Quantisierungsrauschen ist nicht permanent vorhanden, sondern tritt nur in Passagen mit Ton auf.
Das macht es f&uuml;r den H&ouml;rer sehr schwer das Nutzsignal akustisch vom Quantisierungsrauschen zu trennen.
</p>
<h4>3.4</h4>
<p class="aufgabe">
  Modifiziere Dein Programm noch einmal so, dass auch das Differenzsignal zwischen Original und bitreduziertem
  Signal, das hei&szlig;t, das Quantisierungsrauschen ausgegeben werden kann.
  Welchen Charakter hat das Rauschen bei einer Reduktion um 1 bit, wie ver&auml;ndert es sich bei zunehmender
  Bit-Reduktion? (Waves f&uuml;r all diese F&auml;lle ins Protokoll, Ausschnitte als Plots)
</p>
<p class="antwort">
  folgender Code wurde in wave_io eingef&uuml;gt:
<pre>
for(i=0; i &lt; n_wave; i++) {
	newwave[i] = wave[i];
	wave[i] /= pow(2,10);
	wave[i] *= pow(2,10);
	wave[i] -= newwave[i]; //Differenzsignal fuer Aufgabe 3.4
	wave[i] *= pow(2,16-10-2); //Q-Fehler verst&auml;rken
  }
</pre>
  Bei einer Bitreduktion um 1 bit dominiert das Quantisierungsrauschen, jedoch vermag ein marginaler Anteil der Musik
  durch dieses Zufallsdickicht hindurchzuschalmeien. Je h&ouml;her die Bitreduktion, desto h&ouml;her ist der Anteil des
  Originalsignals im Differenzsignal. Daraus l&auml;&szlig;t sich schliessen dass immer gr&ouml;&szlig;ere Informationsanteile aus
  dem Originalsignal verschwinden.<br/><br/>

  <a href="files/youtube_bitreduzierung-diff-2hoch1.wav">Differenzsignal 1 Bit</a><br/>
  <img src="img/diff_2hoch1.jpg"/><br/><br/>
  <a href="files/youtube_bitreduzierung-diff-2hoch4.wav">Differenzsignal 4 Bit</a><br/>
  <img src="img/diff_2hoch4.jpg"/><br/><br/>
  <a href="files/youtube_bitreduzierung-diff-2hoch10.wav">Differenzsignal 10 Bit</a><br/>
  <img src="img/diff_2hoch10.jpg"/><br/><br/>

</p>

  </body>
</html>
